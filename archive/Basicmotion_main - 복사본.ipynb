{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:56.958133Z",
     "start_time": "2023-06-08T15:44:56.925409Z"
    }
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import dataloader\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "importlib.reload(dataloader)\n",
    "from model import *\n",
    "from dataloader import *\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "# from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.005012Z",
     "start_time": "2023-06-08T15:44:56.996038Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"root\" : \"./data/pickles/\",\n",
    "    \"train_feature\" : \"Basicmotion_feature.pickle\",\n",
    "    \"train_target\" : \"Basicmotion_target.pickle\",\n",
    "    \"test_feature\" : \"Basicmotion_feature_test.pickle\",\n",
    "    \"test_target\" : \"Basicmotion_target_test.pickle\",\n",
    "    \"labelencodedict\" : { 'Running' : 0,\n",
    "                          'Walking' : 1,\n",
    "                          'Badminton' : 2,\n",
    "                          'Standing' :3},\n",
    "    \"epochs\" : 3000,\n",
    "    \"learning_rate\" : 1e-4,\n",
    "    # model \n",
    "    \"first_in_channels\" : 6,\n",
    "    \"first_out_channels\" : 32,\n",
    "    \"first_kernel_size\" : 5,\n",
    "    \"second_in_channels\" : 32,\n",
    "    \"second_out_channels\" : 16,\n",
    "    \"second_kernel_size\" : 3,\n",
    "    \"dim_model\" : 64,\n",
    "    \"dim_inner\" : 128,\n",
    "    \"num_heads\" : 8,\n",
    "    \"dropout_rate\" : 0.15,\n",
    "    \"squeeze_factor\" : 2,\n",
    "    \"sec_kernel_size\" : 3,\n",
    "    \"sec_stride\" : 1,\n",
    "    \"N\" : 3,\n",
    "    \"gamma\" : 2,\n",
    "    \"u\" : 2,\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.020971Z",
     "start_time": "2023-06-08T15:44:57.011995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open(config[\"root\"] + config[\"train_feature\"], 'rb') as fr:\n",
    "#     features = pickle.load(fr)\n",
    "# with open(config[\"root\"] + config[\"train_target\"], 'rb') as fr2:\n",
    "#     targets = pickle.load(fr2)\n",
    "# with open(config[\"root\"] + config[\"test_feature\"], 'rb') as fr:\n",
    "#     features_test = pickle.load(fr)\n",
    "# with open(config[\"root\"] + config[\"test_target\"], 'rb') as fr2:\n",
    "#     targets_test = pickle.load(fr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.035930Z",
     "start_time": "2023-06-08T15:44:57.025958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Util Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.051888Z",
     "start_time": "2023-06-08T15:44:57.043911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Util Function\n",
    "# def to_categorical(y, num_classes):\n",
    "#     \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "#     a = np.eye(num_classes)[y]\n",
    "#     return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Making Train Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.067880Z",
     "start_time": "2023-06-08T15:44:57.058869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # append features\n",
    "# X_train = []\n",
    "# y_train = []\n",
    "# for key in features:\n",
    "#     X_train.append(features[key])\n",
    "#     y_train.append(targets[key])\n",
    "\n",
    "# # make it into array\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# # make y_train into 1d vector\n",
    "# tmp = np.squeeze(y_train)\n",
    "\n",
    "# # make one-hot vector\n",
    "# ys = []\n",
    "# for t in tmp:\n",
    "#     ys.append(config['labelencodedict'][t[0]])\n",
    "# train_y = to_categorical(ys, num_classes=4)\n",
    "\n",
    "# # make X_train into tensor\n",
    "# X_train_numpy = X_train.astype(np.float32)\n",
    "# X_train = torch.from_numpy(X_train_numpy)\n",
    "# X_train = torch.Tensor(X_train)\n",
    "# # make y_train into tensor\n",
    "# y_train = torch.Tensor(ys)\n",
    "# # train_y is one-hot vector\n",
    "# train_y = torch.from_numpy(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.083802Z",
     "start_time": "2023-06-08T15:44:57.072834Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_dataset  = CustomDataset(X_train, y_train)\n",
    "# # Define batch size\n",
    "# batch_size = 8\n",
    "\n",
    "# # Create train data loader\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Iterate over the data loader in your training loop\n",
    "# for batch, label in train_dataloader:\n",
    "#     # Perform your training operations here\n",
    "#     print(batch.shape)  # Example operation, printing the batch shape\n",
    "#     print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Making Test Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.100757Z",
     "start_time": "2023-06-08T15:44:57.088789Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # append features\n",
    "# X_test = []\n",
    "# y_test = []\n",
    "# for key in features_test:\n",
    "#     X_test.append(features_test[key])\n",
    "#     y_test.append(targets_test[key])\n",
    "\n",
    "# # make it into array\n",
    "# X_test = np.array(X_test)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "# # make y_test into 1d vector\n",
    "# tmp = np.squeeze(y_test)\n",
    "\n",
    "# # make one-hot vector\n",
    "# ys_test = []\n",
    "# for t in tmp:\n",
    "#     ys_test.append(config['labelencodedict'][t[0]])\n",
    "# test_y = to_categorical(ys_test, num_classes = len(config[\"labelencodedict\"].keys()))\n",
    "\n",
    "\n",
    "# # make X_test into tensor\n",
    "# X_test_numpy = X_test.astype(np.float32)\n",
    "# X_test =torch.from_numpy(X_test_numpy)\n",
    "# X_test = torch.Tensor(X_test)\n",
    "# # make y_test into tensor\n",
    "# y_train = torch.Tensor(ys)\n",
    "# # test_y is one-hot vector\n",
    "# test_y = torch.Tensor(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.131705Z",
     "start_time": "2023-06-08T15:44:57.105744Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "error_train = []\n",
    "def train(train_dataloader, optimizer, model = None, epochs = 30, batch_size=False, verbose = True):\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "#     X, y = Variable(X_train), Variable(train_y)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.\n",
    "        epoch_error = 0.\n",
    "        \n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            batch_X, batch_y = Variable(batch_X), Variable(batch_y)\n",
    "            print(batch_y)\n",
    "            loss = model.calculate_objective(batch_X, batch_y)\n",
    "            loss = loss.mean()\n",
    "            error = model.calculate_classification_error(batch_X, batch_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss#.item()\n",
    "            epoch_error += error#.item()\n",
    "        train_loss.append(epoch_loss / len(train_dataloader))\n",
    "        train_error.append(epoch_error / len(train_dataloader))\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch: {}, Loss: {:.4f}, Train error: {:.4f}'.format(epoch, \n",
    "                                                                        epoch_loss / len(train_dataloader), \n",
    "                                                                       epoch_error / len(train_dataloader)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.163621Z",
     "start_time": "2023-06-08T15:44:57.143675Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss_train = []\n",
    "# error_train = []\n",
    "# def train(X_train, y_train, optimizer, model = None, epochs = 30, batch_size=False, verbose = True):\n",
    "    \n",
    "#     X, y = Variable(X_train), Variable(train_y)\n",
    "#     for epoch in range(epochs):\n",
    "#         train_loss = 0.\n",
    "#         train_error = 0.\n",
    "        \n",
    "#         model.train()\n",
    "#         for batch_X, batch_y in train_dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             loss = model.calculate_objective(X, y)\n",
    "#             loss = loss.mean()\n",
    "#     #         print(model(X))\n",
    "#             error = model.calculate_classification_error(X, y)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         if verbose:\n",
    "#             print('Epoch: {}, Loss: {:.4f}, Train error: {:.4f}'.format(epoch, loss, error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:57.194536Z",
     "start_time": "2023-06-08T15:44:57.173593Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(X_test, y_test, model = None, epochs = 30, batch_size = False):\n",
    "    model.eval()\n",
    "    X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "    loss = model.calculate_objective(X_test, y_test)\n",
    "    loss = loss.mean()\n",
    "    error = model.calculate_classification_error(X_test, y_test)\n",
    "    print('TEST, Loss: {:.4f}, Test error: {:.4f}'.format(loss, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:58.407328Z",
     "start_time": "2023-06-08T15:44:57.201518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 2., 1., 0., 2., 0., 1.])\n",
      "[tensor([ 0.1620, -0.0373,  0.0274, -0.5681,  1.8119, -0.3797, -0.8167,  1.1756,\n",
      "        -1.2328,  0.3728, -0.3563,  0.7763,  1.0979,  0.5555,  0.1158,  0.2629,\n",
      "         0.2655,  0.0816, -1.4559,  0.7499,  2.1749,  0.1409, -0.7384,  0.7371,\n",
      "        -0.6306, -0.7719,  1.6997, -0.2863,  0.5808, -0.6369,  0.2644,  0.8801],\n",
      "       grad_fn=<SliceBackward0>), tensor([-0.0051,  0.4539,  0.6353, -0.5731,  2.0595, -0.6976, -0.0543,  1.5490,\n",
      "        -0.6052, -0.1935,  0.4897, -0.1000,  1.0222,  1.2183,  0.4065, -0.5550,\n",
      "         0.2529,  0.5205, -1.6043,  0.6465,  1.9033,  0.6995, -0.0164,  1.0247,\n",
      "        -0.3869, -0.7728,  1.4752, -0.4032,  0.3569, -0.7497, -0.3159,  0.4214],\n",
      "       grad_fn=<SliceBackward0>), tensor([ 0.1294, -0.2751, -0.1461, -0.1087,  1.4161, -0.0497, -0.2308,  0.5165,\n",
      "        -0.4146,  0.0903,  0.2212,  0.7531,  0.1108, -0.0660,  0.2465, -0.0452,\n",
      "         0.4677,  0.4760, -1.1353,  0.3975,  1.3376,  0.0876, -0.3958,  0.6081,\n",
      "        -0.2220, -1.2181,  0.5382,  0.0690,  0.7907, -1.0939,  0.1695,  0.3316],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.1071, -0.2859, -0.2350, -0.1969,  1.4049, -0.1098, -0.2632,  0.4666,\n",
      "        -0.3622,  0.1514,  0.3118,  0.5913,  0.2614, -0.0958,  0.2071, -0.1094,\n",
      "         0.3057,  0.3691, -1.1104,  0.2178,  1.3133,  0.2617, -0.2734,  0.5268,\n",
      "        -0.2769, -1.2250,  0.6035,  0.0213,  0.9145, -1.1085,  0.1213,  0.3482],\n",
      "       grad_fn=<SliceBackward0>), tensor([-0.2466,  0.5790, -0.2046, -0.3633,  2.1422, -1.0184, -0.4077,  1.4357,\n",
      "        -1.0362, -0.8237,  0.2949,  0.3430,  0.1921,  0.1445,  0.6758, -0.5283,\n",
      "        -0.2205, -0.4260, -2.1492,  0.6428,  2.3529,  0.5740, -0.1470,  1.5029,\n",
      "        -0.9244,  0.0639,  1.0400, -0.8576,  0.2180, -1.0349,  0.3144,  0.7531],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.1052, -0.2193, -0.1692, -0.1263,  1.4988, -0.0300, -0.1821,  0.4632,\n",
      "        -0.4421,  0.0941,  0.1959,  0.5254,  0.1605, -0.0403,  0.3302, -0.2470,\n",
      "         0.3367,  0.4305, -1.1012,  0.2006,  1.4184,  0.1547, -0.3002,  0.5350,\n",
      "        -0.3558, -1.1787,  0.5994,  0.0932,  1.0075, -0.9564,  0.1348,  0.3639],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.2582, -0.1117, -0.1884, -0.1665,  1.4798, -0.1362, -0.3557,  0.4716,\n",
      "        -0.5593, -0.1856,  0.4386,  0.5705,  0.3195,  0.0453,  0.2202, -0.1507,\n",
      "         0.4315,  0.5293, -1.0710,  0.3167,  1.4363,  0.3226, -0.2727,  0.5875,\n",
      "        -0.2866, -1.3196,  0.6423,  0.1478,  0.7959, -1.1653,  0.2698,  0.4038],\n",
      "       grad_fn=<SliceBackward0>), tensor([-0.5489,  1.7110, -0.3586, -0.8968,  2.0062, -0.7273, -0.1490,  2.2995,\n",
      "        -0.6260, -0.6652, -1.1843,  0.1479,  0.6482,  1.8607,  0.8141,  0.0167,\n",
      "        -0.8870, -0.7072, -3.0887,  1.3047,  3.0603,  0.8276,  0.2181,  1.9750,\n",
      "        -1.2860,  0.3493,  2.6157, -0.5920,  0.3457, -0.3462, -1.5837,  0.6821],\n",
      "       grad_fn=<SliceBackward0>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuins\\Hanyang_23_1st\\AI\\model.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_prob = torch.nn.functional.softmax(scaled_att_score)\n",
      "C:\\Users\\yuins\\Hanyang_23_1st\\AI\\model.py:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_prob = torch.nn.functional.softmax(scaled_att_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.2378, -0.0689, -0.0974, -0.3835,  1.5874, -0.1456, -0.8418,  1.2218,\n",
      "        -1.2388,  0.2855, -0.1981,  0.7416,  1.1008,  0.4847,  0.2301,  0.3336,\n",
      "         0.2642,  0.2134, -1.2756,  0.5289,  2.1787,  0.1405, -0.6115,  0.5510,\n",
      "        -0.7693, -1.0135,  1.6090, -0.2465,  0.7359, -0.6369,  0.4104,  0.7484],\n",
      "       grad_fn=<SliceBackward0>), tensor([-0.5080,  1.5069, -0.1401, -0.8765,  2.0511, -0.2084, -0.0749,  2.0198,\n",
      "        -0.7203, -0.3860, -1.0875,  0.4041,  0.6194,  1.7786,  0.6624, -0.0606,\n",
      "        -0.7693, -0.8917, -2.8915,  1.2761,  3.2315,  0.6478,  0.1889,  1.6784,\n",
      "        -1.4739,  0.3298,  2.7382, -0.3977,  0.6368, -0.2018, -1.8448,  0.7518],\n",
      "       grad_fn=<SliceBackward0>), tensor([-0.2768,  0.6531, -0.2212, -0.5221,  2.3583, -0.7925, -0.4083,  1.6544,\n",
      "        -0.8912, -1.1253,  0.4935,  0.2246,  0.1838,  0.4854,  0.7654, -0.6438,\n",
      "        -0.1950, -0.4222, -2.2002,  0.5035,  2.4468,  0.5153,  0.0598,  1.7767,\n",
      "        -1.3179,  0.4366,  1.1945, -0.8785,  0.4322, -1.3753,  0.3040,  0.7952],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.2349, -0.0878, -0.2346, -0.2108,  1.4441, -0.0760, -0.3034,  0.5141,\n",
      "        -0.5412, -0.1626,  0.3823,  0.5704,  0.2764, -0.0571,  0.1559, -0.1362,\n",
      "         0.3694,  0.4201, -1.0158,  0.3097,  1.4884,  0.3923, -0.2097,  0.5462,\n",
      "        -0.2737, -1.3114,  0.5786,  0.2009,  0.7291, -1.2431,  0.2708,  0.4453],\n",
      "       grad_fn=<SliceBackward0>), tensor([ 0.0799,  0.7834,  0.4113, -0.5209,  1.8301, -0.7574, -0.1464,  1.6196,\n",
      "        -0.7549, -0.0627,  0.3295,  0.0706,  0.9795,  0.8643,  0.2584, -0.5229,\n",
      "         0.3422, -0.0737, -1.8809,  0.6875,  2.1776,  0.6931, -0.0670,  0.7406,\n",
      "        -0.4648, -0.6014,  1.5958, -0.3383,  0.3309, -0.7579, -0.3888,  0.6087],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.1687, -0.3026, -0.2873, -0.2188,  1.4183, -0.2160, -0.4408,  0.5024,\n",
      "        -0.4236,  0.0876,  0.3131,  0.6363,  0.2442, -0.1841,  0.2477, -0.0493,\n",
      "         0.2376,  0.3665, -1.0422,  0.1317,  1.3905,  0.2153, -0.2870,  0.5584,\n",
      "        -0.2899, -1.2196,  0.5376,  0.0299,  0.8502, -1.0961,  0.2690,  0.4157],\n",
      "       grad_fn=<SliceBackward0>), tensor([ 0.1494, -0.2483, -0.1347, -0.1879,  1.4560, -0.0533, -0.2812,  0.3524,\n",
      "        -0.5014,  0.1459,  0.2925,  0.6800,  0.1983, -0.1249,  0.2106, -0.1928,\n",
      "         0.2698,  0.2859, -1.0852,  0.1555,  1.3773,  0.2451, -0.3912,  0.4523,\n",
      "        -0.3027, -1.1778,  0.6061,  0.0850,  0.9845, -1.0740,  0.2302,  0.4444],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "[tensor([ 0.1775, -0.2086, -0.0961, -0.1451,  1.4229, -0.0341, -0.1858,  0.4456,\n",
      "        -0.4343,  0.0587,  0.2912,  0.6496,  0.0609, -0.0199,  0.2694, -0.1359,\n",
      "         0.4437,  0.4588, -1.0850,  0.4024,  1.3355,  0.0901, -0.4129,  0.6049,\n",
      "        -0.2561, -1.2176,  0.6108,  0.0717,  0.9028, -1.0536,  0.1456,  0.2932],\n",
      "       grad_fn=<SliceBackward0>)]\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([3., 3., 2., 1., 0., 2., 0., 1.])\n",
      "tensor([2., 2., 2., 0., 1., 2., 3., 0.])\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "tensor([2., 2., 2., 0., 1., 2., 3., 0.])\n",
      "tensor([3., 1., 0., 1., 3., 2., 2., 3.])\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n",
      "[tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], grad_fn=<SliceBackward0>)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-755f1c5a2e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-87b8b1d41d0b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, optimizer, model, epochs, batch_size, verbose)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_classification_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Hanyang_23_1st\\AI\\model.py\u001b[0m in \u001b[0;36mcalculate_classification_error\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_classification_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;31m# print(y_hat)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Hanyang_23_1st\\AI\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# print(\"X shape\", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mprototypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prototype_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# C_all을 구함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[1;31m# x = x.unsqueeze(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# print(\"xrep shape\", x.shape) # 40, 1, 32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Hanyang_23_1st\\AI\\model.py\u001b[0m in \u001b[0;36m_prototype_learning\u001b[1;34m(self, X_rep, y, classes)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prototype_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[0mC_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[1;31m# print(X_rep.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = config[\"epochs\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "#     print(X_train.shape)\n",
    "    train_dataloader, test_dataloader, y_train, y_test = load_dataset(config)\n",
    "    t = [i for i, _ in train_dataloader]\n",
    "    ys = [j for _, j in train_dataloader]\n",
    "#     print(t[0].shape[0])\n",
    "    config[\"num_samples\"] = t[0].shape[0]\n",
    "    config[\"L\"] = t[0].shape[2]\n",
    "    config[\"length\"] = t[0].shape[2]\n",
    "#     print(len(config[\"labelencodedict\"].keys()))\n",
    "    md = Net(\n",
    "                dim_model = config[\"dim_model\"],\n",
    "                gamma = config[\"gamma\"],\n",
    "                u = config[\"u\"],\n",
    "                ys = ys,\n",
    "                first_in_channels = config[\"first_in_channels\"],\n",
    "                first_out_channels = config[\"first_out_channels\"],\n",
    "                first_kernel_size = config[\"first_kernel_size\"],\n",
    "                second_in_channels = config[\"second_in_channels\"],\n",
    "                second_out_channels = config[\"second_out_channels\"],\n",
    "                second_kernel_size = config[\"second_kernel_size\"],\n",
    "                N = config[\"N\"],  #slice\n",
    "                L = config[\"L\"], # slice\n",
    "                dim_inner = config[\"dim_inner\"],\n",
    "                num_samples = config[\"num_samples\"], \n",
    "                length = config[\"length\"], \n",
    "                num_heads = config[\"num_heads\"],\n",
    "                dropout_rate = config[\"dropout_rate\"],\n",
    "                squeeze_factor = config[\"squeeze_factor\"],\n",
    "                sec_kernel_size = config[\"sec_kernel_size\"],\n",
    "                sec_stride = config[\"sec_stride\"],\n",
    "                num_classes = len(config[\"labelencodedict\"].keys()))\n",
    "    # 각 layer의 이름과 파라미터 출력\n",
    "    optimizer = torch.optim.Adam(md.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train(train_dataloader,optimizer=optimizer, model = md, epochs = 1000, verbose = True)\n",
    "    test(X_test, test_y, optimizer=optimizer, model = md, epochs = epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T15:44:58.425246Z",
     "start_time": "2023-06-08T15:44:42.822Z"
    }
   },
   "outputs": [],
   "source": [
    "[i for i in train_dataloader]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
